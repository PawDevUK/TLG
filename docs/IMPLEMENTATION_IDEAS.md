# Time Line Generator (TLG) - Implementation Ideas & Approaches

## Table of Contents
1. [Architecture Approaches](#architecture-approaches)
2. [Technology Stack Recommendations](#technology-stack-recommendations)
3. [LLM Integration Options](#llm-integration-options)
4. [Article Generation Workflow](#article-generation-workflow)
5. [Multi-Repository Handling](#multi-repository-handling)
6. [Deployment Strategies](#deployment-strategies)
7. [Implementation Phases](#implementation-phases)
8. [Code Examples](#code-examples)

---

## Architecture Approaches

### Option 1: Next.js Application (PRIMARY - Recommended)

**Architecture Flow**:
```
User Interface â†’ Next.js App Router â†’ API Routes â†’ Git Parser â†’ OpenAI LLM â†’ MongoDB â†’ Timeline Display
```

**Pros**:
- âœ… Full-featured web interface with React
- âœ… Built-in API routes (no separate backend needed)
- âœ… Server-side rendering for SEO
- âœ… Easy deployment to Vercel (one-click)
- âœ… TypeScript support out of the box
- âœ… Interactive timeline display
- âœ… Real-time article generation
- âœ… User authentication support (NextAuth.js)
- âœ… Modern developer experience

**Cons**:
- âŒ Requires hosting (but Vercel free tier is generous)
- âŒ More initial setup than simple scripts
- âŒ Needs database connection management

**Best For**: Production applications, portfolio integration, team collaboration, interactive features

**This is the PRIMARY approach for Time Line Generator.**

---

### Option 2: GitHub Actions-Based (Alternative)

**Architecture Flow**:
```
Git Push â†’ GitHub Actions Trigger â†’ Parse Commits â†’ Call LLM â†’ Generate Article â†’ POST to API â†’ Store in MongoDB
```

**Pros**:
- âœ… No server infrastructure needed initially
- âœ… Runs automatically on every push
- âœ… Native GitHub integration
- âœ… Free for public repos (2000 min/month for private)
- âœ… Easy to set up and maintain

**Cons**:
- âŒ No web interface
- âŒ 6-hour maximum execution time
- âŒ Limited to GitHub-hosted repos
- âŒ Secrets management required for API keys
- âŒ Can be costly for very active repos

**Best For**: Background processing, automated summaries without UI

---

### Option 3: Local Scheduled Task (Alternative)

**Architecture Flow**:
```
Cron Job â†’ Scan Local Repos â†’ Parse Git Changes â†’ Call LLM â†’ Generate Article â†’ POST to API â†’ Store in MongoDB
```

**Pros**:
- âœ… Works with any git repository (GitHub, GitLab, local)
- âœ… No GitHub Actions quota limits
- âœ… Full control over execution environment
- âœ… Can use local LLM (no API costs)

**Cons**:
- âŒ Requires always-on machine or server
- âŒ Manual setup for each machine
- âŒ Need to handle authentication to remote repos
- âŒ More complex deployment

**Best For**: Power users, multiple repository sources, cost-conscious users

---

### Option 3: Hybrid Cloud Service

**Architecture Flow**:
```
Webhook â†’ Cloud Function â†’ Parse Commits â†’ Queue Job â†’ Worker Processes â†’ LLM Generation â†’ Store in DB
```

**Pros**:
- âœ… Scalable to many repositories
- âœ… Works with multiple git providers
- âœ… Professional-grade reliability
- âœ… Async processing with queues
- âœ… Easy monitoring and logging

**Cons**:
- âŒ More complex architecture
- âŒ Higher operational costs
- âŒ Requires cloud infrastructure
- âŒ Overkill for personal use

**Best For**: Teams, enterprise use, product offering

---

## Technology Stack Recommendations

### Option A: JavaScript/TypeScript Full Stack (Recommended)

```yaml
Frontend: React/Next.js (already in use in Portfolio)
Backend: Node.js + Express
Database: MongoDB Atlas
Runtime: Node.js 20+
Package Manager: npm or pnpm
LLM: OpenAI API
Deployment: Vercel (frontend) + Railway/Render (backend)
CI/CD: GitHub Actions
```

**Why This Stack**:
- Consistent language across stack
- Already familiar (based on Portfolio projects)
- Excellent ecosystem and tooling
- TypeScript provides type safety
- Easy deployment options

**Package Dependencies**:
```json
{
  "dependencies": {
    "express": "^4.18.2",
    "mongoose": "^8.0.0",
    "openai": "^4.20.0",
    "simple-git": "^3.20.0",
    "dotenv": "^16.3.0",
    "axios": "^1.6.0",
    "date-fns": "^2.30.0"
  },
  "devDependencies": {
    "@types/express": "^4.17.20",
    "@types/node": "^20.0.0",
    "typescript": "^5.2.0",
    "ts-node": "^10.9.0",
    "nodemon": "^3.0.0"
  }
}
```

---

### Option B: Python-Based (Alternative)

```yaml
Backend: FastAPI or Flask
Database: MongoDB (pymongo) or PostgreSQL
LLM: OpenAI API or Langchain
Git Integration: GitPython
Deployment: Railway, Fly.io, or AWS Lambda
```

**Why Python**:
- Excellent AI/ML ecosystem
- Native LLM library support (Langchain)
- Great for data processing
- Strong git parsing libraries

**Not Recommended Because**:
- Adds language complexity (portfolio is JavaScript)
- Less integration with existing Portfolio codebase

---

## LLM Integration Options

### Option 1: OpenAI ChatGPT API (Recommended)

**Implementation**:
```javascript
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

async function generateArticle(commits, diffs) {
  const prompt = `
    Analyze these git changes and create a professional work summary:
    
    Commits: ${JSON.stringify(commits)}
    File Changes: ${diffs}
    
    Generate a detailed paragraph describing:
    1. What was implemented
    2. Technical decisions made
    3. Challenges overcome
    4. Results achieved
    
    Use professional, technical language. Be specific about files and changes.
  `;
  
  const response = await openai.chat.completions.create({
    model: "gpt-4-turbo-preview",
    messages: [{ role: "user", content: prompt }],
    temperature: 0.7,
    max_tokens: 1000
  });
  
  return response.choices[0].message.content;
}
```

**Pros**:
- âœ… High-quality output
- âœ… Fast response times (2-5 seconds)
- âœ… No local compute needed
- âœ… Regular model improvements
- âœ… Easy to implement

**Cons**:
- âŒ Ongoing API costs (~$0.01-0.03 per article with GPT-4)
- âŒ Requires internet connection
- âŒ Data sent to OpenAI servers
- âŒ Rate limiting (10-60 RPM depending on tier)

**Cost Estimate**:
- GPT-4 Turbo: ~$0.02 per article
- Daily active development: ~$0.60/month
- 100 articles/month: ~$2.00/month

---

### Option 2: Local LLM (Ollama)

**Implementation**:
```javascript
import axios from 'axios';

async function generateArticle(commits, diffs) {
  const prompt = `Analyze these git changes...`;
  
  const response = await axios.post('http://localhost:11434/api/generate', {
    model: 'llama2',
    prompt: prompt,
    stream: false
  });
  
  return response.data.response;
}
```

**Setup**:
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Download model
ollama pull llama2

# Or use larger models
ollama pull mistral
ollama pull codellama
```

**Pros**:
- âœ… Zero API costs
- âœ… Complete privacy
- âœ… Unlimited usage
- âœ… Works offline
- âœ… Customizable models

**Cons**:
- âŒ Requires powerful hardware (8GB+ RAM)
- âŒ Slower generation (10-30 seconds)
- âŒ Lower quality than GPT-4
- âŒ More complex setup

**Best For**: Cost-sensitive, privacy-focused, high-volume usage

---

### Option 3: Anthropic Claude API

**Implementation**:
```javascript
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

async function generateArticle(commits, diffs) {
  const message = await anthropic.messages.create({
    model: "claude-3-opus-20240229",
    max_tokens: 1024,
    messages: [{ role: "user", content: prompt }]
  });
  
  return message.content[0].text;
}
```

**Pros**:
- âœ… High quality output (comparable to GPT-4)
- âœ… Good at technical content
- âœ… Longer context window
- âœ… Competitive pricing

**Cons**:
- âŒ Similar costs to OpenAI
- âŒ Less popular (smaller ecosystem)

---

## Article Generation Workflow

### Detailed Step-by-Step Process

#### Step 1: Git Change Collection
```javascript
import simpleGit from 'simple-git';

async function getRecentCommits(repoPath, since) {
  const git = simpleGit(repoPath);
  
  // Get commits from last 24 hours
  const log = await git.log({
    since: since,
    '--no-merges': null
  });
  
  return log.all;
}

async function getCommitDiff(repoPath, commitHash) {
  const git = simpleGit(repoPath);
  const diff = await git.show([commitHash, '--stat', '--format=']);
  return diff;
}
```

#### Step 2: Data Aggregation
```javascript
async function aggregateWorkData(repos, date) {
  const workData = [];
  
  for (const repo of repos) {
    const commits = await getRecentCommits(repo.path, date);
    
    const changes = {
      repoName: repo.name,
      commits: commits.map(c => ({
        hash: c.hash,
        message: c.message,
        author: c.author_name,
        date: c.date,
        files: c.diff?.files || []
      }))
    };
    
    workData.push(changes);
  }
  
  return workData;
}
```

#### Step 3: LLM Prompt Construction
```javascript
function buildPrompt(workData) {
  const promptParts = [
    "Generate a professional work summary for the following development work:",
    "",
    "IMPORTANT: Write in past tense, third person perspective.",
    "Include specific technical details (file names, functions, configs).",
    "Mention challenges and how they were resolved.",
    "",
  ];
  
  workData.forEach(repo => {
    promptParts.push(`\n## Repository: ${repo.repoName}`);
    promptParts.push(`Commits: ${repo.commits.length}`);
    
    repo.commits.forEach(commit => {
      promptParts.push(`\n### ${commit.message}`);
      promptParts.push(`Files changed: ${commit.files.join(', ')}`);
    });
  });
  
  promptParts.push("\nGenerate a cohesive paragraph (200-400 words) summarizing this work.");
  
  return promptParts.join('\n');
}
```

#### Step 4: Article Generation
```javascript
async function generateDailyArticle(repos, date, period) {
  // 1. Collect git data
  const workData = await aggregateWorkData(repos, date);
  
  // 2. Build prompt
  const prompt = buildPrompt(workData);
  
  // 3. Call LLM
  const description = await generateArticle(workData, prompt);
  
  // 4. Structure article
  const article = {
    title: repos.length === 1 ? repos[0].name : "Multiple Projects",
    date: formatDate(date),
    updates: [{
      period: period,
      description: description
    }]
  };
  
  return article;
}
```

#### Step 5: API Submission
```javascript
import { postTimelineEntry } from './timelineClient';

async function submitArticle(article) {
  try {
    const result = await postTimelineEntry(article);
    console.log('âœ“ Article submitted successfully:', result);
    return result;
  } catch (error) {
    console.error('âœ— Failed to submit article:', error.message);
    throw error;
  }
}
```

---

## Multi-Repository Handling

### Strategy 1: Per-Repository Articles

**Approach**: Generate separate article for each repository

```javascript
const repos = [
  { name: 'Portfolio Frontend', path: '/path/to/repo1' },
  { name: 'Portfolio Server', path: '/path/to/repo2' },
  { name: 'TLG', path: '/path/to/repo3' }
];

async function generateArticles(date, period) {
  const articles = [];
  
  for (const repo of repos) {
    const workData = await aggregateWorkData([repo], date);
    
    // Skip if no changes
    if (workData[0].commits.length === 0) continue;
    
    const article = await generateDailyArticle([repo], date, period);
    articles.push(article);
  }
  
  return articles;
}
```

**Pros**: Clear separation, easy to attribute work
**Cons**: Multiple API calls, separate entries

---

### Strategy 2: Consolidated Multi-Repo Article

**Approach**: Combine multiple repositories into single article

```javascript
async function generateConsolidatedArticle(repos, date, period) {
  const allWorkData = await aggregateWorkData(repos, date);
  
  // Filter repos with changes
  const activeRepos = allWorkData.filter(r => r.commits.length > 0);
  
  if (activeRepos.length === 0) return null;
  
  const prompt = buildMultiRepoPrompt(activeRepos);
  const description = await generateArticle(activeRepos, prompt);
  
  return {
    title: "Development Work",
    date: formatDate(date),
    updates: [{
      period: period,
      description: description
    }]
  };
}
```

**Pros**: Single article, holistic view
**Cons**: May lose per-repo granularity

---

### Strategy 3: Smart Grouping

**Approach**: Group related repositories together

```javascript
const repoGroups = {
  'Portfolio': [
    { name: 'Portfolio Frontend', path: '/path/to/frontend' },
    { name: 'Portfolio Server', path: '/path/to/server' }
  ],
  'TLG': [
    { name: 'TLG', path: '/path/to/tlg' }
  ]
};

async function generateGroupedArticles(date, period) {
  const articles = [];
  
  for (const [groupName, repos] of Object.entries(repoGroups)) {
    const workData = await aggregateWorkData(repos, date);
    const activeRepos = workData.filter(r => r.commits.length > 0);
    
    if (activeRepos.length === 0) continue;
    
    const article = await generateDailyArticle(activeRepos, date, period);
    article.title = groupName;
    articles.push(article);
  }
  
  return articles;
}
```

**Pros**: Balance between separation and consolidation
**Cons**: Requires configuration

---

## Deployment Strategies

### GitHub Actions Implementation

**File**: `.github/workflows/generate-timeline.yml`

```yaml
name: Generate Timeline Article

on:
  push:
    branches: [ main, master ]
  schedule:
    # Run at 11 PM every day
    - cron: '0 23 * * *'
  workflow_dispatch: # Manual trigger

jobs:
  generate-article:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 100 # Get commit history
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      
      - name: Install dependencies
        run: npm ci
      
      - name: Generate article
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          API_ENDPOINT: ${{ secrets.API_ENDPOINT }}
          API_KEY: ${{ secrets.API_KEY }}
        run: node src/generate-article.js
      
      - name: Upload artifact (optional)
        uses: actions/upload-artifact@v3
        with:
          name: generated-article
          path: output/article.json
```

**Key Features**:
- Triggers on push to main branch
- Scheduled daily run
- Manual trigger option
- Secure secrets management
- Artifact storage

---

### Local Cron Implementation

**Setup Script**: `setup-cron.sh`

```bash
#!/bin/bash

# Install crontab entry
SCRIPT_PATH="$(pwd)/run-tlg.sh"
CRON_SCHEDULE="0 23 * * *" # 11 PM daily

# Add to crontab
(crontab -l 2>/dev/null; echo "$CRON_SCHEDULE $SCRIPT_PATH >> /var/log/tlg.log 2>&1") | crontab -

echo "âœ“ Cron job installed: $CRON_SCHEDULE"
```

**Execution Script**: `run-tlg.sh`

```bash
#!/bin/bash

# Load environment
export $(cat .env | xargs)

# Change to project directory
cd /path/to/tlg

# Run article generation
node src/generate-article.js

# Check exit code
if [ $? -eq 0 ]; then
  echo "âœ“ Article generated successfully at $(date)"
else
  echo "âœ— Article generation failed at $(date)"
  exit 1
fi
```

---

## Implementation Phases

### Phase 1: Core Setup (Week 1)

**Objectives**:
- Set up project structure
- Configure dependencies
- Create configuration system

**Tasks**:
1. Initialize npm project
2. Install core dependencies
3. Create `.env.example` with required variables
4. Set up TypeScript configuration
5. Create basic folder structure

**Deliverables**:
- `package.json` with all dependencies
- Project structure with src/, config/, tests/ folders
- Configuration loading system
- README with setup instructions

---

### Phase 2: Git Integration (Week 1-2)

**Objectives**:
- Parse git repositories
- Extract commit data
- Calculate statistics

**Tasks**:
1. Create repository scanner
2. Implement commit parsing
3. Extract file changes
4. Calculate additions/deletions
5. Handle multiple repositories

**Deliverables**:
- Working git parser module
- Unit tests for git operations
- Sample output data

---

### Phase 3: LLM Integration (Week 2)

**Objectives**:
- Integrate chosen LLM
- Create effective prompts
- Generate quality articles

**Tasks**:
1. Set up OpenAI/Ollama client
2. Design prompt templates
3. Implement article generation
4. Add retry logic and error handling
5. Test output quality

**Deliverables**:
- LLM client module
- Prompt engineering templates
- Generated sample articles
- Quality validation

---

### Phase 4: Backend API (Week 2-3)

**Objectives**:
- Create REST API
- Implement database storage
- Handle authentication

**Tasks**:
1. Set up Express server
2. Create MongoDB schemas
3. Implement CRUD endpoints
4. Add authentication middleware
5. Deploy to cloud service

**Deliverables**:
- REST API with endpoints
- MongoDB database
- API documentation
- Deployed backend service

---

### Phase 5: Automation (Week 3)

**Objectives**:
- Automate article generation
- Schedule regular runs
- Deploy automation

**Tasks**:
1. Create GitHub Actions workflow
2. OR Set up local cron job
3. Configure secrets/environment
4. Test automated runs
5. Add monitoring/alerting

**Deliverables**:
- Working automation
- Scheduled execution
- Error notifications
- Logs and monitoring

---

### Phase 6: Integration & Testing (Week 4)

**Objectives**:
- Integrate with Portfolio
- End-to-end testing
- Performance optimization

**Tasks**:
1. Update Portfolio frontend to consume API
2. Test multi-repository scenarios
3. Performance testing
4. Security audit
5. Documentation

**Deliverables**:
- Integrated system
- Test coverage report
- Performance benchmarks
- Complete documentation

---

## Code Examples

### Complete Generate Script

**File**: `src/generate-article.js`

```javascript
#!/usr/bin/env node

import dotenv from 'dotenv';
import { format, subDays } from 'date-fns';
import { getRecentCommits, getCommitStats } from './git-parser.js';
import { generateArticle } from './llm-client.js';
import { postTimelineEntry } from './api-client.js';

dotenv.config();

// Configuration
const REPOS = process.env.REPOS.split(',').map(path => ({
  name: path.split('/').pop(),
  path: path.trim()
}));

const PERIOD = process.env.PERIOD || determinePeriod();

function determinePeriod() {
  const hour = new Date().getHours();
  if (hour < 12) return 'Morning';
  if (hour < 17) return 'Afternoon';
  if (hour < 21) return 'Evening';
  return 'Night';
}

async function main() {
  console.log('ðŸš€ Starting TLG article generation...');
  
  try {
    // Get today's date
    const today = new Date();
    const since = subDays(today, 1);
    
    // Collect git data from all repos
    console.log(`ðŸ“š Scanning ${REPOS.length} repositories...`);
    const repoData = [];
    
    for (const repo of REPOS) {
      console.log(`  - ${repo.name}`);
      const commits = await getRecentCommits(repo.path, since);
      
      if (commits.length === 0) {
        console.log(`    âŠ˜ No commits found`);
        continue;
      }
      
      const stats = await getCommitStats(repo.path, commits);
      
      repoData.push({
        name: repo.name,
        commits: commits,
        stats: stats
      });
      
      console.log(`    âœ“ ${commits.length} commits, ${stats.filesChanged} files`);
    }
    
    if (repoData.length === 0) {
      console.log('âŠ˜ No changes detected. Exiting.');
      return;
    }
    
    // Generate article with LLM
    console.log('ðŸ¤– Generating article with LLM...');
    const description = await generateArticle(repoData);
    console.log(`âœ“ Article generated (${description.length} chars)`);
    
    // Structure article
    const article = {
      title: repoData.length === 1 ? repoData[0].name : 'Development Work',
      date: format(today, 'dd/MM/yy'),
      updates: [{
        period: PERIOD,
        description: description
      }]
    };
    
    // Submit to API
    console.log('ðŸ“¤ Submitting to API...');
    const result = await postTimelineEntry(article);
    console.log('âœ“ Article submitted successfully!');
    console.log(`   ID: ${result._id || 'N/A'}`);
    
    console.log('\nâœ¨ TLG completed successfully!');
    
  } catch (error) {
    console.error('\nâŒ Error:', error.message);
    console.error(error.stack);
    process.exit(1);
  }
}

main();
```

### Git Parser Module

**File**: `src/git-parser.js`

```javascript
import simpleGit from 'simple-git';

export async function getRecentCommits(repoPath, since) {
  const git = simpleGit(repoPath);
  
  const log = await git.log({
    since: since.toISOString(),
    '--no-merges': null,
    '--author': process.env.GIT_AUTHOR || null
  });
  
  return log.all.map(commit => ({
    hash: commit.hash,
    message: commit.message,
    author: commit.author_name,
    date: commit.date,
    body: commit.body
  }));
}

export async function getCommitStats(repoPath, commits) {
  const git = simpleGit(repoPath);
  
  let totalInsertions = 0;
  let totalDeletions = 0;
  const filesChanged = new Set();
  
  for (const commit of commits) {
    const diff = await git.show([
      commit.hash,
      '--stat',
      '--format='
    ]);
    
    // Parse diff stats
    const lines = diff.split('\n');
    for (const line of lines) {
      if (line.includes('|')) {
        const [file] = line.split('|');
        filesChanged.add(file.trim());
      }
      
      const match = line.match(/(\d+) insertion.*?(\d+) deletion/);
      if (match) {
        totalInsertions += parseInt(match[1]);
        totalDeletions += parseInt(match[2]);
      }
    }
  }
  
  return {
    commits: commits.length,
    filesChanged: filesChanged.size,
    insertions: totalInsertions,
    deletions: totalDeletions
  };
}

export async function getFileDiff(repoPath, commitHash) {
  const git = simpleGit(repoPath);
  const diff = await git.show([commitHash]);
  return diff;
}
```

### LLM Client Module

**File**: `src/llm-client.js`

```javascript
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

const SYSTEM_PROMPT = `You are a professional technical writer summarizing software development work.

Guidelines:
- Write in past tense, professional tone
- Include specific technical details (file names, functions, configurations)
- Mention technologies and tools used
- Describe both what was done and why
- Include challenges and solutions
- Use technical vocabulary appropriately
- Write 200-400 words
- Create one cohesive paragraph`;

export async function generateArticle(repoData) {
  const prompt = buildPrompt(repoData);
  
  try {
    const response = await openai.chat.completions.create({
      model: process.env.LLM_MODEL || "gpt-4-turbo-preview",
      messages: [
        { role: "system", content: SYSTEM_PROMPT },
        { role: "user", content: prompt }
      ],
      temperature: 0.7,
      max_tokens: 1000
    });
    
    return response.choices[0].message.content.trim();
    
  } catch (error) {
    console.error('LLM API error:', error.message);
    throw new Error(`Failed to generate article: ${error.message}`);
  }
}

function buildPrompt(repoData) {
  const parts = ['Summarize the following development work:\n'];
  
  repoData.forEach(repo => {
    parts.push(`\n## ${repo.name}`);
    parts.push(`Commits: ${repo.stats.commits}`);
    parts.push(`Files changed: ${repo.stats.filesChanged}`);
    parts.push(`Lines: +${repo.stats.insertions} -${repo.stats.deletions}`);
    parts.push('\nCommit messages:');
    
    repo.commits.forEach(commit => {
      parts.push(`- ${commit.message}`);
      if (commit.body) {
        parts.push(`  ${commit.body}`);
      }
    });
  });
  
  return parts.join('\n');
}
```

### API Client Module

**File**: `src/api-client.js`

```javascript
import axios from 'axios';

const API_BASE_URL = process.env.API_ENDPOINT || 'http://localhost:3000';
const API_KEY = process.env.API_KEY;

const client = axios.create({
  baseURL: API_BASE_URL,
  headers: {
    'Content-Type': 'application/json',
    'Authorization': `Bearer ${API_KEY}`
  }
});

export async function postTimelineEntry(article) {
  try {
    const response = await client.post('/api/timeline', article);
    return response.data.data;
  } catch (error) {
    if (error.response) {
      throw new Error(`API error: ${error.response.status} - ${error.response.data.error || 'Unknown error'}`);
    }
    throw new Error(`Network error: ${error.message}`);
  }
}

export async function getTimeline(limit = 10) {
  try {
    const response = await client.get('/api/timeline', {
      params: { limit }
    });
    return response.data.data;
  } catch (error) {
    throw new Error(`Failed to fetch timeline: ${error.message}`);
  }
}
```

---

## Configuration Files

### package.json

```json
{
  "name": "timeline-generator",
  "version": "0.1.0",
  "description": "Automated GitHub repository change tracker and article generator",
  "main": "src/index.js",
  "type": "module",
  "scripts": {
    "start": "node src/generate-article.js",
    "dev": "nodemon src/generate-article.js",
    "test": "NODE_OPTIONS=--experimental-vm-modules jest",
    "lint": "eslint src/**/*.js",
    "server": "node src/server/index.js"
  },
  "keywords": ["git", "timeline", "article", "automation", "llm"],
  "author": "PawDevUK",
  "license": "MIT",
  "dependencies": {
    "axios": "^1.6.2",
    "date-fns": "^2.30.0",
    "dotenv": "^16.3.1",
    "express": "^4.18.2",
    "mongoose": "^8.0.3",
    "openai": "^4.20.1",
    "simple-git": "^3.20.0"
  },
  "devDependencies": {
    "@types/express": "^4.17.21",
    "@types/node": "^20.10.5",
    "eslint": "^8.55.0",
    "jest": "^29.7.0",
    "nodemon": "^3.0.2",
    "typescript": "^5.3.3"
  }
}
```

### .env.example

```bash
# Git Configuration
REPOS=/path/to/repo1,/path/to/repo2,/path/to/repo3
GIT_AUTHOR=Your Name

# LLM Configuration
OPENAI_API_KEY=sk-...
LLM_MODEL=gpt-4-turbo-preview

# API Configuration
API_ENDPOINT=https://your-api.com
API_KEY=your-api-key-here

# Optional
PERIOD=Afternoon
LOG_LEVEL=info
```

### .gitignore

```
node_modules/
.env
.env.local
*.log
output/
dist/
.DS_Store
coverage/
.vscode/
```

---

## Recommendations Summary

### For MVP (Next 2 Weeks):
1. âœ… **Use GitHub Actions** for automation (easiest to start)
2. âœ… **Use OpenAI ChatGPT API** (best quality/effort ratio)
3. âœ… **JavaScript/TypeScript stack** (consistent with Portfolio)
4. âœ… **MongoDB Atlas** (free tier sufficient)
5. âœ… **Start with single repository** (iterate quickly)

### For Production:
1. Add comprehensive error handling
2. Implement retry logic with exponential backoff
3. Add monitoring and alerting
4. Cost tracking for LLM usage
5. Caching for frequently accessed data
6. Rate limiting protection
7. Backup strategy for generated articles

### For Scale:
1. Consider job queue (Bull, BullMQ)
2. Implement webhook-based triggers
3. Add support for GitLab, Bitbucket
4. Create web UI for configuration
5. Multi-user support
6. Analytics dashboard

---

## Next Steps

1. **Review and decide** on architecture approach
2. **Choose LLM provider** (OpenAI recommended)
3. **Create initial project structure** with package.json
4. **Implement git parser module** (core functionality)
5. **Integrate LLM** and test article generation
6. **Create backend API** for article storage
7. **Set up automation** (GitHub Actions or cron)
8. **Test end-to-end** with real repositories
9. **Deploy and monitor**
10. **Iterate and improve** based on results

**Estimated Time to MVP**: 2-3 weeks of focused development

Good luck! ðŸš€
